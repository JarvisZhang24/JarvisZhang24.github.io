---
title: "Vision-Language-Action for Robotics"
tech_stack: "PyTorch, ROS, OpenAI Gym, Simulation Environments"
status: "In Progress"
order: 3
---

Exploring Vision-Language-Action (VLA) architectures that enable robots to understand natural language instructions and execute complex manipulation tasks based on visual observations.

**Research Focus:**
- End-to-end learning from visual and linguistic inputs to robot actions
- Transfer learning from simulation to real-world scenarios
- Sample-efficient reinforcement learning for robot manipulation
- Grounding natural language in physical environments

**Technical Approach:**
- Leveraging pre-trained vision-language models as policy backbones
- Incorporating reinforcement learning for action optimization
- Multi-task learning across diverse manipulation scenarios
- Real-world robot experiments with feedback loops

**Goal:** Create intelligent robotic systems that can understand and execute complex tasks described in natural language, making robots more accessible and versatile.
