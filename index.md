---
layout: home
description: Yichen Zhang — M.S. student in Computer Engineering at NYU. Research in Vision-Language Models, Reinforcement Learning, and Embodied AI.
---

<div class="profile-section">
  <img src="/assets/profile.png" alt="Yichen Zhang" class="profile-photo">
  <div class="profile-info">
    <h1>Yichen Zhang</h1>
    <p class="affiliation">
      M.S. in Computer Engineering, <a href="https://engineering.nyu.edu/">New York University</a><br>
      Research: Vision-Language Models, Reinforcement Learning, Embodied AI
    </p>
    <div class="profile-links">
      <a href="mailto:jarviszhang.ai@gmail.com">Email</a>
      <a href="https://github.com/JarvisZhang24">GitHub</a>
      <a href="https://scholar.google.com/">Google Scholar</a>
      <a href="/assets/cv.pdf">CV</a>
    </div>
  </div>
</div>

I am a graduate student in the Department of Electrical and Computer Engineering at **New York University**, where I focus on building intelligent systems that bridge vision, language, and action. My research interests span **vision-language models**, **reinforcement learning**, and **embodied AI** for robotics.

Previously, I obtained my B.S. in Communication Engineering from **Beijing Jiaotong University**.

---

<h2 class="section-title">Research Interests</h2>

<div class="research-grid">
  <div class="research-card">
    <h3>Vision-Language Models</h3>
    <p>Multimodal understanding and reasoning across visual and linguistic modalities, with applications in VLM fine-tuning and evaluation.</p>
  </div>
  <div class="research-card">
    <h3>Reinforcement Learning</h3>
    <p>Sample-efficient algorithms, policy optimization, and reward shaping for autonomous decision-making in complex environments.</p>
  </div>
  <div class="research-card">
    <h3>Embodied AI & Robotics</h3>
    <p>Vision-Language-Action frameworks that enable robots to perceive, reason, and interact with the physical world.</p>
  </div>
</div>

---

<h2 class="section-title">News</h2>

<ul class="news-list">
  <li><span class="news-date">Feb 2025</span> Started working on Vision-Language-Action architectures for robotic manipulation.</li>
  <li><span class="news-date">Jan 2025</span> Began fine-tuning large vision-language models on H100 GPUs for multimodal reasoning.</li>
  <li><span class="news-date">Sep 2024</span> Joined NYU as an M.S. student in Computer Engineering.</li>
</ul>

---

<h2 class="section-title">Selected Projects</h2>

<div class="project-card">
  <h3>FastVLM — Vision-Language Model Fine-Tuning</h3>
  <div class="project-desc">Fine-tuning large vision-language models on H100 GPUs for enhanced multimodal reasoning and real-world applications.</div>
  <div class="tech-stack"><span>PyTorch</span> <span>Hugging Face</span> <span>CUDA</span> <span>Multi-GPU</span></div>
</div>

<div class="project-card">
  <h3>VariantAI — Genome-Scale AI Platform</h3>
  <div class="project-desc">A genome-scale model integration platform combining AI with biological data analysis for precision medicine applications.</div>
  <div class="tech-stack"><span>Python</span> <span>Next.js</span> <span>Bioinformatics</span> <span>Cloud Computing</span></div>
</div>

<div class="project-card">
  <h3>Vision-Language-Action Systems for Robotics</h3>
  <div class="project-desc">Exploring VLA architectures for robotic manipulation, enabling robots to understand and act based on visual and linguistic inputs.</div>
  <div class="tech-stack"><span>PyTorch</span> <span>ROS</span> <span>Transformers</span></div>
</div>

<p style="margin-top: 0.5rem;"><a href="/projects/">View all projects →</a></p>
