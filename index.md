---
layout: home
description: Yichen Zhang â€” M.S. student in Computer Engineering at NYU. Research in Vision-Language Models, Reinforcement Learning, and Embodied AI.
---

<div class="profile-section">
  <img src="/assets/profile.png" alt="Yichen Zhang" class="profile-photo">
  <div class="profile-info">
    <h1>Yichen Zhang</h1>
    <p class="affiliation">
      M.S. in Computer Engineering, <a href="https://engineering.nyu.edu/">New York University</a><br>
      Research: Vision-Language Models, Reinforcement Learning, Embodied AI
    </p>
    <div class="profile-links">
      <a href="https://github.com/JarvisZhang24">GitHub</a>
      <a href="https://scholar.google.com/">Google Scholar</a>
      <a href="/assets/cv.pdf">CV</a>
    </div>
  </div>
</div>

<p class="bio-text">
I am a graduate student in the Department of Electrical and Computer Engineering at <strong>New York University</strong>, where I focus on building intelligent systems that bridge vision, language, and action. My research interests span <strong>vision-language models</strong>, <strong>reinforcement learning</strong>, and <strong>embodied AI</strong> for robotics.
</p>

<p class="bio-text">
Previously, I obtained my B.S. in Communication Engineering from <strong>Beijing Jiaotong University</strong>.
</p>

---

<h2 class="section-heading">Research Interests</h2>

<ul class="research-list">
  <li><strong>Vision-Language Models</strong> &mdash; Multimodal understanding and reasoning across visual and linguistic modalities, with applications in VLM fine-tuning and evaluation.</li>
  <li><strong>Reinforcement Learning</strong> &mdash; Sample-efficient algorithms, policy optimization, and reward shaping for autonomous decision-making in complex environments.</li>
  <li><strong>Embodied AI &amp; Robotics</strong> &mdash; Vision-Language-Action frameworks that enable robots to perceive, reason, and interact with the physical world.</li>
</ul>

---

<h2 class="section-heading">News</h2>

<table class="news-table">
  <tr><td class="news-date">Feb 2025</td><td>Started working on Vision-Language-Action architectures for robotic manipulation.</td></tr>
  <tr><td class="news-date">Jan 2025</td><td>Began fine-tuning large vision-language models on H100 GPUs for multimodal reasoning.</td></tr>
  <tr><td class="news-date">Sep 2024</td><td>Joined NYU as an M.S. student in Computer Engineering.</td></tr>
</table>
